{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Do you remember what the regression is?\n",
    "\n",
    "*Nasza odpowiedź*\n",
    "Regresja - metoda statystyczna pozwalająca na opisanie współzmienności kilku zmiennych przez dopasowanie do nich funkcji. Umożliwia przewidywanie nieznanych wartości jednych wielkości na podstawie znanych wartości innych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today's analysis we will run on dataset 'car-mpg' which has the following columns: cylinders, displacement, horsepower, weight, acceleration, model year, origin and mpg. Mpg is decision attribute and stands for miles per gallon, which basically means the reverse of combustion (pl. spalanie). The dataset comes from https://archive.ics.uci.edu/ml/datasets/auto+mpg However, it does not have the original data, but normalized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import f, f_oneway\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "     0         1         2         3         4    5    6     7\n0  1.0  0.617571  0.456522  0.536150  0.238095  0.0  0.0  18.0\n1  1.0  0.728682  0.646739  0.589736  0.208333  0.0  0.0  15.0\n2  1.0  0.645995  0.565217  0.516870  0.178571  0.0  0.0  18.0\n3  1.0  0.609819  0.565217  0.516019  0.238095  0.0  0.0  16.0\n4  1.0  0.604651  0.510870  0.520556  0.148810  0.0  0.0  17.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>0.617571</td>\n      <td>0.456522</td>\n      <td>0.536150</td>\n      <td>0.238095</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>18.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>0.728682</td>\n      <td>0.646739</td>\n      <td>0.589736</td>\n      <td>0.208333</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>15.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>0.645995</td>\n      <td>0.565217</td>\n      <td>0.516870</td>\n      <td>0.178571</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>18.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>0.609819</td>\n      <td>0.565217</td>\n      <td>0.516019</td>\n      <td>0.238095</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>16.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>0.604651</td>\n      <td>0.510870</td>\n      <td>0.520556</td>\n      <td>0.148810</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>17.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('car-mpg.csv', header=None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the first step of the task let's get to know with our data and its distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year', 'origin', 'mpg']\n",
    "\n",
    "# TODO Iterate through the columns (attributes) in our dataset and draw their histograms. \n",
    "# Which attributes can be potentially more interesting in further analysis? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIMPLE LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** Write your own function which calculates linear regression step by step and plot the results. <br/>\n",
    "*Hint* If you do not remember how it was calculated, look here: https://en.wikipedia.org/wiki/Simple_linear_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearRegressionOne(x, y):\n",
    "    #TODO write your linear regression function\n",
    "    mean_x = np.mean(x)\n",
    "    mean_y = np.mean(y)\n",
    "\n",
    "    numerator = np.sum((x - mean_x) * (y - mean_y))\n",
    "    denominator = np.sum((x - mean_x) ** 2)\n",
    "\n",
    "    a = numerator / denominator\n",
    "    b = mean_y - a * mean_x\n",
    "\n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-28.860049132825637 32.45397418066428\n"
     ]
    }
   ],
   "source": [
    "a, b = LinearRegressionOne(data[2], data[7]) # TODO run your function on attributes 'horsepower' and 'mpg'\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** As you already wrote your own function which calculates simple linear regression, you can now use implementation from sklearn package. This time split your data to train and test set (use first 40 observations as test ones) and plot your results. Compare the results from your own implementation and this one from sklearn. Did you get similar coefficients for your linear function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO simple linear regression with the usage of sklearn\n",
    "def LinearRegressionOneScikit(x, y):\n",
    "    \n",
    "    return \n",
    "\n",
    "LinearRegressionOneScikit(data[2], data[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not have to limit to only simple line but we also can create some other shapes. Run the following codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julka\\AppData\\Local\\Temp\\ipykernel_31644\\4190084107.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x['x^2']=x[2]**2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Feature names are only supported if all input features have string names, but your input has ['int', 'str'] as feature name / column name types. If you want feature names to be stored and validated, you must convert them all to strings, by using X.columns = X.columns.astype(str) for example. Otherwise you can remove feature / column names from your input data, or convert them all to a non-string data type.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 7\u001B[0m\n\u001B[0;32m      2\u001B[0m y \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39miloc[:,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m      4\u001B[0m x[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mx^2\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m=\u001B[39mx[\u001B[38;5;241m2\u001B[39m]\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m\n\u001B[1;32m----> 7\u001B[0m reg \u001B[38;5;241m=\u001B[39m \u001B[43mlinear_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLinearRegression\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43my\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m score \u001B[38;5;241m=\u001B[39m reg\u001B[38;5;241m.\u001B[39mscore(x,y)\n\u001B[0;32m     10\u001B[0m x_line \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(np\u001B[38;5;241m.\u001B[39mlinspace(\u001B[38;5;241m0\u001B[39m,\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m100\u001B[39m))\n",
      "File \u001B[1;32m~\\PycharmProjects\\NN-ML-zadanka\\venv\\Lib\\site-packages\\sklearn\\base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1471\u001B[0m     )\n\u001B[0;32m   1472\u001B[0m ):\n\u001B[1;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\NN-ML-zadanka\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:609\u001B[0m, in \u001B[0;36mLinearRegression.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    605\u001B[0m n_jobs_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_jobs\n\u001B[0;32m    607\u001B[0m accept_sparse \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpositive \u001B[38;5;28;01melse\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsr\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsc\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcoo\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m--> 609\u001B[0m X, y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    610\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    611\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    612\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maccept_sparse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    613\u001B[0m \u001B[43m    \u001B[49m\u001B[43my_numeric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    614\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmulti_output\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    615\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforce_writeable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    616\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    618\u001B[0m has_sw \u001B[38;5;241m=\u001B[39m sample_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    619\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_sw:\n",
      "File \u001B[1;32m~\\PycharmProjects\\NN-ML-zadanka\\venv\\Lib\\site-packages\\sklearn\\base.py:608\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001B[0m\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_validate_data\u001B[39m(\n\u001B[0;32m    538\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    539\u001B[0m     X\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mno_validation\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    544\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params,\n\u001B[0;32m    545\u001B[0m ):\n\u001B[0;32m    546\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001B[39;00m\n\u001B[0;32m    547\u001B[0m \n\u001B[0;32m    548\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    606\u001B[0m \u001B[38;5;124;03m        validated.\u001B[39;00m\n\u001B[0;32m    607\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 608\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_feature_names\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    610\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_tags()[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrequires_y\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m    611\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    612\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m estimator \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    613\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrequires y to be passed, but the target y is None.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    614\u001B[0m         )\n",
      "File \u001B[1;32m~\\PycharmProjects\\NN-ML-zadanka\\venv\\Lib\\site-packages\\sklearn\\base.py:469\u001B[0m, in \u001B[0;36mBaseEstimator._check_feature_names\u001B[1;34m(self, X, reset)\u001B[0m\n\u001B[0;32m    449\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Set or check the `feature_names_in_` attribute.\u001B[39;00m\n\u001B[0;32m    450\u001B[0m \n\u001B[0;32m    451\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.0\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    465\u001B[0m \u001B[38;5;124;03m       should set `reset=False`.\u001B[39;00m\n\u001B[0;32m    466\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    468\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m reset:\n\u001B[1;32m--> 469\u001B[0m     feature_names_in \u001B[38;5;241m=\u001B[39m \u001B[43m_get_feature_names\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    470\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m feature_names_in \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    471\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeature_names_in_ \u001B[38;5;241m=\u001B[39m feature_names_in\n",
      "File \u001B[1;32m~\\PycharmProjects\\NN-ML-zadanka\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2279\u001B[0m, in \u001B[0;36m_get_feature_names\u001B[1;34m(X)\u001B[0m\n\u001B[0;32m   2277\u001B[0m \u001B[38;5;66;03m# mixed type of string and non-string is not supported\u001B[39;00m\n\u001B[0;32m   2278\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(types) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstr\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m types:\n\u001B[1;32m-> 2279\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[0;32m   2280\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFeature names are only supported if all input features have string names, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2281\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut your input has \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtypes\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m as feature name / column name types. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2282\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIf you want feature names to be stored and validated, you must convert \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2283\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthem all to strings, by using X.columns = X.columns.astype(str) for \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2284\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexample. Otherwise you can remove feature / column names from your input \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2285\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata, or convert them all to a non-string data type.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2286\u001B[0m     )\n\u001B[0;32m   2288\u001B[0m \u001B[38;5;66;03m# Only feature names of all strings are supported\u001B[39;00m\n\u001B[0;32m   2289\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(types) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m types[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstr\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[1;31mTypeError\u001B[0m: Feature names are only supported if all input features have string names, but your input has ['int', 'str'] as feature name / column name types. If you want feature names to be stored and validated, you must convert them all to strings, by using X.columns = X.columns.astype(str) for example. Otherwise you can remove feature / column names from your input data, or convert them all to a non-string data type."
     ]
    }
   ],
   "source": [
    "x = data.iloc[:,2:3]\n",
    "y = data.iloc[:,-1]\n",
    "\n",
    "x['x^2']=x[2]**2\n",
    "\n",
    "\n",
    "reg = linear_model.LinearRegression().fit(x,y )\n",
    "score = reg.score(x,y)\n",
    "\n",
    "x_line = np.array(np.linspace(0,1,100))\n",
    "y_line = reg.coef_[1]*x_line**2+ reg.coef_[0]*x_line+ reg.intercept_\n",
    "\n",
    "plt.scatter(x[2],y)\n",
    "plt.plot(x_line,y_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.iloc[:,4:5]\n",
    "y = data.iloc[:,-1]\n",
    "\n",
    "x['x^2']=x[4]**2\n",
    "x['x^3']=x[4]**3\n",
    "\n",
    "reg = linear_model.LinearRegression().fit(x,y )\n",
    "score = reg.score(x,y)\n",
    "\n",
    "x_line = np.array(np.linspace(0,1,100))\n",
    "y_line =  reg.coef_[2]*x_line**3+ reg.coef_[1]*x_line**2+ reg.coef_[0]*x_line+ reg.intercept_\n",
    "\n",
    "plt.scatter(x[4],y)\n",
    "plt.plot(x_line,y_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MULTIPLE LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Do you remember how the multiple linear regression works? <br/>\n",
    "\n",
    "*Nasza odpowiedź*\n",
    "\n",
    "Regresja wieloraka - zmienna objaśniana zależy od więcej niż jednej zmiennej, oblicza się macierzą"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRADIENT DECENT ALGORITHM\n",
    "Gradient descent algorithm’s main objective is to minimise the cost function. It is one of the best optimisation algorithms to minimise errors (difference of actual value and predicted value). In a real world example, it is similar to find out a best direction to take a step downhill.\n",
    "\n",
    "**Task** This time you are about to fill missing values in multiple linear regression implementation with the usage of batch gradient decent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.iloc[:,:-1]\n",
    "y = data.iloc[:,-1:]\n",
    "\n",
    "# we add intercept to the vector of x to make it easier to processwith other values\n",
    "x['intercept'] = 1 \n",
    "\n",
    "n_iterations = 0 # TODO set the number of iterations\n",
    "\n",
    "learning_rate = 0.0 # TODO set learning rate  \n",
    "\n",
    "weights = np.zeros((x.shape[1], 1))\n",
    "\n",
    "print(x.shape[0])\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    y_pred = np.dot(x, weights)\n",
    "    residuals = y_pred - y\n",
    "    gradient_vector = np.dot(x.T, residuals)\n",
    "    weights -= learning_rate/x.shape[0] * gradient_vector\n",
    "\n",
    "# last weight is the value of intercept \n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** You do not need to write your own implementation of multiple linear regression, you can also use sklearn implementation. This time your function should have params: independent variables *x* and dependent variable *y* and return (or just print) score and coefficients of the function (.coef_ and .intercept_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultipleLinearRegression(x, y):\n",
    "    # TODO here goes your code\n",
    "    \n",
    "\n",
    "MultipleLinearRegression(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Compare the results from two above methods. Are they similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIGNIFICANCE REGRESSION COEFFICIENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can statistically check if the model (or specific parameter) is significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GLOBAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** Implement global test for checking significance. Fill the following methods according to your knowledge from the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSSR(y,y_pred):\n",
    "    #TODO\n",
    "    return \n",
    "\n",
    "def getSSE(y,y_pred):\n",
    "    #TODO\n",
    "    return \n",
    "\n",
    "def getF(y, y_pred, n, m):\n",
    "    #TODO\n",
    "    return \n",
    "\n",
    "def getSignificanceRegressionCoefficient(x,y):\n",
    "    #TODO\n",
    "    return F,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = data.iloc[:,:-1].values\n",
    "yy = data.iloc[:,-1:].values[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F,p = getSignificanceRegressionCoefficient(xx,yy)\n",
    "print(F)\n",
    "if p < 0.05:\n",
    "    print(\" confirm \" + str(p))\n",
    "else:\n",
    "    print(\" reject \" + str(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the correctness of your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_stats = sm.OLS(yy, sm.add_constant(xx)).fit()\n",
    "print(sm_stats.fvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOCAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** Now check the significance of each criterion. As the equation for global version is more general, we can use the function that was previously written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTLIERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** What is an outlier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the boxplot below. The points which are at the top and bottom of chart are outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(3, 6))\n",
    "plt.boxplot(data[4], 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method that is used in this type of charts to determine outliers bases on quartiles. Firstly, we need to calculate IQR which stands for interquartile range and is the difference between the third and the first quartile. Usually stating the limit of whiskers is: [Q1-IQR * 1.5, Q3 + IQR * 1.5]. All values that are outside this range are treated as outliers. It is connected with normal distribution and distance greater than 3 std dev."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this method cannot cope correctly with multidimensional data. There can be outliers that are still close to median but overall are far from other points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use *statsmodel* package which calculates a few interesting measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year', 'origin', 'mpg']\n",
    "\n",
    "model = sm.OLS(data.iloc[:,-1], data.iloc[:,:-1])\n",
    "results = model.fit()\n",
    "influence = results.get_influence()\n",
    "sm_fr = influence.summary_frame()\n",
    "sm_fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** Visualize residuals (*standard_resid*) with the usage of scatterplot. Draw also horizontal line on y-value = 0. Look at the chart. Are these values in a narrow row on y-value or you can identify that there are some outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DFFITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This measure calculates the influence of i-th example on our regression equation by omitting this observation and comparing this model to previous one. Boundary values for our influential observations are +/- 2 * sqrt(1/number of observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** Draw a plot where you visualise values of dffits as vertical lines (see: plt.vlines()) and horizontal lines (plt.axhline()) equal to boundaries of influential observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** Get indices of those observations that are influential for diffits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COOK'S DISTANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing to the previous, there is also made checking the influence of each observation. However, this time we check the difference of coefficients in our regression equation while we omit the i-th observation. The boundary of the influence is very often set to 4/number of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** Once again draw a plot where you visualise values of cooks_d as vertical lines and horizontal line equal to boundary of influential observations. *Tip* It might be useful to visualize square root of the values to make the chart more transparent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** Get indices of those observations that are influential for Cook's distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** Check if there are elements that appear on both lists. Maybe one list is subset of the second?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
