{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Do you remember what the regression is?\n",
    "\n",
    "*Nasza odpowiedź*\n",
    "Regresja - metoda statystyczna pozwalająca na opisanie współzmienności kilku zmiennych przez dopasowanie do nich funkcji. Umożliwia przewidywanie nieznanych wartości jednych wielkości na podstawie znanych wartości innych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today's analysis we will run on dataset 'car-mpg' which has the following columns: cylinders, displacement, horsepower, weight, acceleration, model year, origin and mpg. Mpg is decision attribute and stands for miles per gallon, which basically means the reverse of combustion (pl. spalanie). The dataset comes from https://archive.ics.uci.edu/ml/datasets/auto+mpg However, it does not have the original data, but normalized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import f, f_oneway\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('car-mpg.csv', header=None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the first step of the task let's get to know with our data and its distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year', 'origin', 'mpg']\n",
    "# done Iterate through the columns (attributes) in our dataset and draw their histograms.\n",
    "for col in range(len(columns)):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(data[col], bins=20, color='blue', alpha=0.7)\n",
    "    plt.title(f'{columns[col]}')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "# Which attributes can be potentially more interesting in further analysis? \n",
    "# todo\n",
    "data.columns = columns #na potem, żeby było po nazwach a nie indeksach ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIMPLE LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** Write your own function which calculates linear regression step by step and plot the results. <br/>\n",
    "*Hint* If you do not remember how it was calculated, look here: https://en.wikipedia.org/wiki/Simple_linear_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearRegressionOne(x, y):\n",
    "    #done write your linear regression function\n",
    "    a = np.sum((x - np.mean(x)) * (y - np.mean(y))) / np.sum((x - np.mean(x)) ** 2)\n",
    "    b = np.mean(y) - a * np.mean(x)\n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = LinearRegressionOne(data[\"horsepower\"], data[\"mpg\"]) # done run your function on attributes 'horsepower' and 'mpg'\n",
    "Y =  b + a * data[\"horsepower\"]\n",
    "plt.plot(data[\"horsepower\"], Y, label=f'Linear fit: y = {b:.2f} + {a:.2f}x')\n",
    "plt.scatter(data[\"horsepower\"], data[\"mpg\"], color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** As you already wrote your own function which calculates simple linear regression, you can now use implementation from sklearn package. This time split your data to train and test set (use first 40 observations as test ones) and plot your results. Compare the results from your own implementation and this one from sklearn. Did you get similar coefficients for your linear function?\n",
    "\n",
    "*similar? - The same. Well, 0.000000000000004 off, so... **the same***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# TODO simple linear regression with the usage of sklearn\n",
    "def LinearRegressionOneScikit(x_train, y_train):\n",
    "    x_train_reshaped = x_train.values.reshape(-1, 1)\n",
    "    y_train_reshaped = y_train.values.reshape(-1, 1)\n",
    "    model = LinearRegression()\n",
    "    model.fit(x_train_reshaped, y_train_reshaped)\n",
    "    a = model.coef_[0][0]\n",
    "    b = model.intercept_[0]\n",
    "    return a, b\n",
    "\n",
    "X_train, X_test = data[\"horsepower\"][:40], data[\"horsepower\"][40:]\n",
    "Y_train, Y_test = data[\"mpg\"][:40], data[\"mpg\"][40:]\n",
    "a_custom, b_custom = LinearRegressionOne(X_train, Y_train)\n",
    "a_sklearn, b_sklearn = LinearRegressionOneScikit(X_train, Y_train)\n",
    "\n",
    "Y_custom =  b_custom + a_custom * data[\"horsepower\"]\n",
    "Y_sklearn = b_sklearn + a_sklearn * data[\"horsepower\"]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_test, Y_test, color=\"lightblue\", label=\"Test Data\")\n",
    "plt.scatter(X_train, Y_train, color=\"aqua\", label=\"Train Data\")\n",
    "plt.plot(data[\"horsepower\"], Y_custom, label=f'Linear fit: y = {b:.2f} + {a:.2f}x', color=\"pink\")\n",
    "plt.plot(data[\"horsepower\"], Y_sklearn, label=f'Linear fit: y = {b:.2f} + {a:.2f}x', color=\"red\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Custom Implementation Coefficients: \\n  Slope (a): {a_custom}, Intercept (b): {b_custom}\")\n",
    "print(f\"Sklearn Implementation Coefficients: \\n  Slope (a): {a_sklearn}, Intercept (b): {b_sklearn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not have to limit to only simple line but we also can create some other shapes. Run the following codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "x = data.iloc[:,2:3]\n",
    "y = data.iloc[:,-1]\n",
    "\n",
    "x[10] = x[2]**2  # 'x^2'\n",
    "\n",
    "reg = linear_model.LinearRegression().fit(x.to_numpy(),y)\n",
    "score = reg.score(x,y)\n",
    "\n",
    "x_line = np.array(np.linspace(0,1,100))\n",
    "y_line = reg.coef_[1]*x_line**2+ reg.coef_[0]*x_line+ reg.intercept_\n",
    "\n",
    "plt.scatter(x[2],y)\n",
    "plt.plot(x_line,y_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.iloc[:,4:5]\n",
    "y = data.iloc[:,-1]\n",
    "\n",
    "x[10]=x[4]**2  # 'x^2'\n",
    "x[11]=x[4]**3  # 'x^3'\n",
    "\n",
    "reg = linear_model.LinearRegression().fit(x,y)\n",
    "score = reg.score(x,y)\n",
    "\n",
    "x_line = np.array(np.linspace(0,1,100))\n",
    "y_line =  reg.coef_[2]*x_line**3+ reg.coef_[1]*x_line**2+ reg.coef_[0]*x_line+ reg.intercept_\n",
    "\n",
    "plt.scatter(x[4],y)\n",
    "plt.plot(x_line,y_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MULTIPLE LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Do you remember how the multiple linear regression works? <br/>\n",
    "\n",
    "*Nasza odpowiedź*\n",
    "\n",
    "Regresja wieloraka - zmienna objaśniana zależy od więcej niż jednej zmiennej, oblicza się macierzą"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRADIENT DECENT ALGORITHM\n",
    "Gradient descent algorithm’s main objective is to minimise the cost function. It is one of the best optimisation algorithms to minimise errors (difference of actual value and predicted value). In a real world example, it is similar to find out a best direction to take a step downhill.\n",
    "\n",
    "**Task** This time you are about to fill missing values in multiple linear regression implementation with the usage of batch gradient decent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.iloc[:,:-1]\n",
    "y = data.iloc[:,-1:]\n",
    "\n",
    "# we add intercept to the vector of x to make it easier to process with other values\n",
    "x['intercept'] = 1 \n",
    "\n",
    "n_iterations = 1000 # TODO set the number of iterations\n",
    "learning_rate = 0.1 # TODO set learning rate\n",
    "\n",
    "weights = np.zeros((x.shape[1], 1))\n",
    "\n",
    "print(x.shape[0])\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    y_pred = np.dot(x, weights)\n",
    "    residuals = y_pred - y\n",
    "    gradient_vector = np.dot(x.T, residuals)\n",
    "    weights -= learning_rate/x.shape[0] * gradient_vector\n",
    "\n",
    "# last weight is the value of intercept \n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** You do not need to write your own implementation of multiple linear regression, you can also use sklearn implementation. This time your function should have params: independent variables *x* and dependent variable *y* and return (or just print) score and coefficients of the function (.coef_ and .intercept_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultipleLinearRegression(x_train, y_train):\n",
    "    # DONE here goes your code\n",
    "    model = LinearRegression()\n",
    "    model.fit(x_train, y_train)\n",
    "    return model.coef_, model.intercept_\n",
    "    \n",
    "\n",
    "\n",
    "coefficients, intercept, model = MultipleLinearRegression(x_train, y_train)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Coefficients:\", coefficients)\n",
    "print(\"Intercept:\", intercept)\n",
    "print(f\"R^2 Score on Test Set: {r2:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Compare the results from two above methods. Are they similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIGNIFICANCE REGRESSION COEFFICIENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can statistically check if the model (or specific parameter) is significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GLOBAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** Implement global test for checking significance. Fill the following methods according to your knowledge from the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSSR(y,y_pred):\n",
    "    #TODO\n",
    "    return \n",
    "\n",
    "def getSSE(y,y_pred):\n",
    "    #TODO\n",
    "    return \n",
    "\n",
    "def getF(y, y_pred, n, m):\n",
    "    #TODO\n",
    "    return \n",
    "\n",
    "def getSignificanceRegressionCoefficient(x,y):\n",
    "    #TODO\n",
    "    return F,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = data.iloc[:,:-1].values\n",
    "yy = data.iloc[:,-1:].values[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F,p = getSignificanceRegressionCoefficient(xx,yy)\n",
    "print(F)\n",
    "if p < 0.05:\n",
    "    print(\" confirm \" + str(p))\n",
    "else:\n",
    "    print(\" reject \" + str(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the correctness of your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_stats = sm.OLS(yy, sm.add_constant(xx)).fit()\n",
    "print(sm_stats.fvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOCAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** Now check the significance of each criterion. As the equation for global version is more general, we can use the function that was previously written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTLIERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** What is an outlier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the boxplot below. The points which are at the top and bottom of chart are outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(3, 6))\n",
    "plt.boxplot(data[4], 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method that is used in this type of charts to determine outliers bases on quartiles. Firstly, we need to calculate IQR which stands for interquartile range and is the difference between the third and the first quartile. Usually stating the limit of whiskers is: [Q1-IQR * 1.5, Q3 + IQR * 1.5]. All values that are outside this range are treated as outliers. It is connected with normal distribution and distance greater than 3 std dev."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this method cannot cope correctly with multidimensional data. There can be outliers that are still close to median but overall are far from other points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use *statsmodel* package which calculates a few interesting measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year', 'origin', 'mpg']\n",
    "\n",
    "model = sm.OLS(data.iloc[:,-1], data.iloc[:,:-1])\n",
    "results = model.fit()\n",
    "influence = results.get_influence()\n",
    "sm_fr = influence.summary_frame()\n",
    "sm_fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** Visualize residuals (*standard_resid*) with the usage of scatterplot. Draw also horizontal line on y-value = 0. Look at the chart. Are these values in a narrow row on y-value or you can identify that there are some outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DFFITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This measure calculates the influence of i-th example on our regression equation by omitting this observation and comparing this model to previous one. Boundary values for our influential observations are +/- 2 * sqrt(1/number of observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** Draw a plot where you visualise values of dffits as vertical lines (see: plt.vlines()) and horizontal lines (plt.axhline()) equal to boundaries of influential observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** Get indices of those observations that are influential for diffits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COOK'S DISTANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing to the previous, there is also made checking the influence of each observation. However, this time we check the difference of coefficients in our regression equation while we omit the i-th observation. The boundary of the influence is very often set to 4/number of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** Once again draw a plot where you visualise values of cooks_d as vertical lines and horizontal line equal to boundary of influential observations. *Tip* It might be useful to visualize square root of the values to make the chart more transparent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** Get indices of those observations that are influential for Cook's distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** Check if there are elements that appear on both lists. Maybe one list is subset of the second?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
