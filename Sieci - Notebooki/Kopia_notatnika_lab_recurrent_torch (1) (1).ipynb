{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebvqJaNU9bkH"
      },
      "source": [
        "# Wprowadzenie do sieci neuronowych i uczenia maszynowego - Sieci Rekurencyjne\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Prowadzący:** Piotr Baryczkowski, Jakub Bednarek<br>\n",
        "**Kontakt:** piotr.baryczkowski@put.poznan.pl<br>\n",
        "\n",
        "---\n",
        "\n",
        "Zadania wypełnione przez \\\\\n",
        "Wojciech Kot 151879 \\\\\n",
        "Julia Samp 151775 \\\\\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wlq47LA0BuBB"
      },
      "source": [
        "## Cel ćwiczeń:\n",
        "- zapoznanie się z rekurencyjnymi sieciami neuronowymi,\n",
        "- stworzenie modelu sieci z warstwami rekurencyjnymi dla zbioru danych MNIST,\n",
        "- stworzenie własnych implementacji warstwami neuronowych"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxLU8paIDmUe",
        "outputId": "ce058942-6b62-4336-93e2-751da14ed3f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.5.1+cu121\n",
            "CUDA available: False\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mzTzdZHPfEkv"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wV_u-YBWEJ8X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cee70115-edb0-4cb5-ce22-4865886c5472"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 52.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 2.01MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 14.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 3.60MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "training_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppmDSGoyFuJ9"
      },
      "source": [
        "## Sieci rekurencyjne\n",
        "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
        "\n",
        "http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
        "\n",
        "http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
        "\n",
        "Przykładowy model z warstwą rekurencyjną dla danych MNIST:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_YKDA5VkfEkw",
        "outputId": "157270eb-4a7c-4ee1-912d-7c99bd021709",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RecurrentModel(\n",
              "  (lstm_1): LSTM(28, 128, batch_first=True)\n",
              "  (relu_1): ReLU()\n",
              "  (dense_1): Linear(in_features=128, out_features=10, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "class RecurrentModel(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(RecurrentModel, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        # Define your layers here.\n",
        "        self.lstm_1 = nn.LSTM(input_size=28, hidden_size=128, batch_first=True)\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.dense_1 = nn.Linear(128, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if inputs.dim() == 4:\n",
        "            # Example: (batch_size, channels, sequence_length, features)\n",
        "            inputs = inputs.squeeze(1)  # Remove the channels dimension if it's 1\n",
        "        elif inputs.dim() != 3:\n",
        "            raise ValueError(f\"Expected input to be 3D, got {inputs.dim()}D instead.\")\n",
        "\n",
        "        lstm_out, _ = self.lstm_1(inputs)\n",
        "        # Take the last output from the sequence (assume inputs are padded appropriately or have consistent lengths)\n",
        "        x = lstm_out[:, -1, :]  # Get the output of the last time step\n",
        "        x = self.relu_1(x)\n",
        "        x = self.dense_1(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "model = RecurrentModel(num_classes=10)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EgXZxT7SfEkw"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 32\n",
        "epochs = 5\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * batch_size + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "t-L7ZTjWfEkx",
        "outputId": "50d892cf-febb-498b-9a9e-51484f9223ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.302975  [   32/60000]\n",
            "loss: 2.174590  [ 3232/60000]\n",
            "loss: 1.936720  [ 6432/60000]\n",
            "loss: 1.930533  [ 9632/60000]\n",
            "loss: 1.624184  [12832/60000]\n",
            "loss: 1.725207  [16032/60000]\n",
            "loss: 1.613878  [19232/60000]\n",
            "loss: 1.673230  [22432/60000]\n",
            "loss: 1.705237  [25632/60000]\n",
            "loss: 1.677566  [28832/60000]\n",
            "loss: 1.567096  [32032/60000]\n",
            "loss: 1.705653  [35232/60000]\n",
            "loss: 1.812220  [38432/60000]\n",
            "loss: 1.567553  [41632/60000]\n",
            "loss: 1.550892  [44832/60000]\n",
            "loss: 1.571253  [48032/60000]\n",
            "loss: 1.505547  [51232/60000]\n",
            "loss: 1.604810  [54432/60000]\n",
            "loss: 1.539813  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 92.5%, Avg loss: 1.539122 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.528945  [   32/60000]\n",
            "loss: 1.549627  [ 3232/60000]\n",
            "loss: 1.549197  [ 6432/60000]\n",
            "loss: 1.556629  [ 9632/60000]\n",
            "loss: 1.600389  [12832/60000]\n",
            "loss: 1.572724  [16032/60000]\n",
            "loss: 1.493337  [19232/60000]\n",
            "loss: 1.523466  [22432/60000]\n",
            "loss: 1.553421  [25632/60000]\n",
            "loss: 1.576199  [28832/60000]\n",
            "loss: 1.584434  [32032/60000]\n",
            "loss: 1.503972  [35232/60000]\n",
            "loss: 1.571354  [38432/60000]\n",
            "loss: 1.508636  [41632/60000]\n",
            "loss: 1.494077  [44832/60000]\n",
            "loss: 1.564033  [48032/60000]\n",
            "loss: 1.488711  [51232/60000]\n",
            "loss: 1.495644  [54432/60000]\n",
            "loss: 1.526677  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 93.4%, Avg loss: 1.528478 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.492315  [   32/60000]\n",
            "loss: 1.496402  [ 3232/60000]\n",
            "loss: 1.613189  [ 6432/60000]\n",
            "loss: 1.585611  [ 9632/60000]\n",
            "loss: 1.502681  [12832/60000]\n",
            "loss: 1.576039  [16032/60000]\n",
            "loss: 1.566568  [19232/60000]\n",
            "loss: 1.554103  [22432/60000]\n",
            "loss: 1.492622  [25632/60000]\n",
            "loss: 1.646859  [28832/60000]\n",
            "loss: 1.544054  [32032/60000]\n",
            "loss: 1.529926  [35232/60000]\n",
            "loss: 1.523439  [38432/60000]\n",
            "loss: 1.467461  [41632/60000]\n",
            "loss: 1.523923  [44832/60000]\n",
            "loss: 1.463383  [48032/60000]\n",
            "loss: 1.587280  [51232/60000]\n",
            "loss: 1.557943  [54432/60000]\n",
            "loss: 1.530766  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.7%, Avg loss: 1.505194 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.524912  [   32/60000]\n",
            "loss: 1.516570  [ 3232/60000]\n",
            "loss: 1.520705  [ 6432/60000]\n",
            "loss: 1.651425  [ 9632/60000]\n",
            "loss: 1.492441  [12832/60000]\n",
            "loss: 1.510164  [16032/60000]\n",
            "loss: 1.521300  [19232/60000]\n",
            "loss: 1.461475  [22432/60000]\n",
            "loss: 1.461435  [25632/60000]\n",
            "loss: 1.492495  [28832/60000]\n",
            "loss: 1.553149  [32032/60000]\n",
            "loss: 1.532210  [35232/60000]\n",
            "loss: 1.492156  [38432/60000]\n",
            "loss: 1.571792  [41632/60000]\n",
            "loss: 1.479478  [44832/60000]\n",
            "loss: 1.465021  [48032/60000]\n",
            "loss: 1.525238  [51232/60000]\n",
            "loss: 1.527236  [54432/60000]\n",
            "loss: 1.472392  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.4%, Avg loss: 1.498491 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.478941  [   32/60000]\n",
            "loss: 1.491312  [ 3232/60000]\n",
            "loss: 1.492466  [ 6432/60000]\n",
            "loss: 1.470968  [ 9632/60000]\n",
            "loss: 1.493639  [12832/60000]\n",
            "loss: 1.559031  [16032/60000]\n",
            "loss: 1.523872  [19232/60000]\n",
            "loss: 1.527533  [22432/60000]\n",
            "loss: 1.484570  [25632/60000]\n",
            "loss: 1.480507  [28832/60000]\n",
            "loss: 1.491788  [32032/60000]\n",
            "loss: 1.516589  [35232/60000]\n",
            "loss: 1.523699  [38432/60000]\n",
            "loss: 1.470325  [41632/60000]\n",
            "loss: 1.492084  [44832/60000]\n",
            "loss: 1.461553  [48032/60000]\n",
            "loss: 1.508451  [51232/60000]\n",
            "loss: 1.492477  [54432/60000]\n",
            "loss: 1.492588  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.9%, Avg loss: 1.492666 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgtZzVYg1361"
      },
      "source": [
        "### Zadanie 1\n",
        "Rozszerz model z powyższego przykładu o kolejną warstwę rekurencyjną przed gęstą warstwą wyjściową.\n",
        "\n",
        "Standardowe sieci neuronowe generują jeden wynik na podstawie jednego inputu.\n",
        "Natomiast sieci rekurencyjne przetwarzają dane sekwencyjnie, w każdym kroku łącząc wynik poprzedniego przetwarzania i aktualnego wejścia. Dlatego domyślnym wejściem sieci neuronowej jest tensor 3-wymiarowy ([batch_size,sequence_size,sample_size]).\n",
        "Domyślnie warstwy rekurencyjne w PyTorchu zwracają sekwencje wyników wszystkich kroków przetwarzania dla warstwy rekurencyjnej. Jeśli chcesz zwrócić tylko wyniki ostatniego przetwarzania dla warstwy rekurencyjnej, musisz samemu to zaimplementować np. `x = lstm_out[:, -1, :]`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FzMsg5A7fEky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f887f18a-c9c4-4a77-b29c-78661d8e3a38"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RecurrentModel2(\n",
              "  (lstm_1): LSTM(28, 128, batch_first=True)\n",
              "  (relu_1): ReLU()\n",
              "  (lstm_2): LSTM(128, 128, batch_first=True)\n",
              "  (relu_2): ReLU()\n",
              "  (dense_1): Linear(in_features=128, out_features=10, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "class RecurrentModel2(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(RecurrentModel2, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        # Define your layers here.\n",
        "        self.lstm_1 = nn.LSTM(input_size=28, hidden_size=128, batch_first=True)\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.lstm_2 = nn.LSTM(input_size=128, hidden_size=128, batch_first=True)\n",
        "        self.relu_2 = nn.ReLU()\n",
        "        self.dense_1 = nn.Linear(128, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if inputs.dim() == 4:\n",
        "            # Example: (batch_size, channels, sequence_length, features)\n",
        "            inputs = inputs.squeeze(1)  # Remove the channels dimension if it's 1\n",
        "        elif inputs.dim() != 3:\n",
        "            raise ValueError(f\"Expected input to be 3D, got {inputs.dim()}D instead.\")\n",
        "\n",
        "        # TODO forward\n",
        "        lstm_out_1, _ = self.lstm_1(inputs)\n",
        "        lstm_out_2, _ = self.lstm_2(lstm_out_1)\n",
        "        x = lstm_out_2[:, -1, :]  # Get the output of the last time step\n",
        "        x = self.relu_1(x)\n",
        "        x = self.dense_1(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "model = RecurrentModel2(num_classes=10)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3ptuv6IHfEky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "314fccc7-bfb4-48a2-83d7-b63c50752123"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.302902  [   32/60000]\n",
            "loss: 1.948869  [ 3232/60000]\n",
            "loss: 1.778781  [ 6432/60000]\n",
            "loss: 1.735029  [ 9632/60000]\n",
            "loss: 1.681669  [12832/60000]\n",
            "loss: 1.619261  [16032/60000]\n",
            "loss: 1.733459  [19232/60000]\n",
            "loss: 1.657458  [22432/60000]\n",
            "loss: 1.647344  [25632/60000]\n",
            "loss: 1.555269  [28832/60000]\n",
            "loss: 1.509418  [32032/60000]\n",
            "loss: 1.603885  [35232/60000]\n",
            "loss: 1.540100  [38432/60000]\n",
            "loss: 1.625782  [41632/60000]\n",
            "loss: 1.525551  [44832/60000]\n",
            "loss: 1.577795  [48032/60000]\n",
            "loss: 1.514037  [51232/60000]\n",
            "loss: 1.561574  [54432/60000]\n",
            "loss: 1.508332  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 94.3%, Avg loss: 1.519068 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.536653  [   32/60000]\n",
            "loss: 1.462890  [ 3232/60000]\n",
            "loss: 1.493048  [ 6432/60000]\n",
            "loss: 1.498760  [ 9632/60000]\n",
            "loss: 1.493448  [12832/60000]\n",
            "loss: 1.534859  [16032/60000]\n",
            "loss: 1.585794  [19232/60000]\n",
            "loss: 1.501599  [22432/60000]\n",
            "loss: 1.494671  [25632/60000]\n",
            "loss: 1.492572  [28832/60000]\n",
            "loss: 1.528761  [32032/60000]\n",
            "loss: 1.502297  [35232/60000]\n",
            "loss: 1.461538  [38432/60000]\n",
            "loss: 1.492854  [41632/60000]\n",
            "loss: 1.468665  [44832/60000]\n",
            "loss: 1.511955  [48032/60000]\n",
            "loss: 1.587772  [51232/60000]\n",
            "loss: 1.531596  [54432/60000]\n",
            "loss: 1.466843  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.9%, Avg loss: 1.493387 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.461247  [   32/60000]\n",
            "loss: 1.555372  [ 3232/60000]\n",
            "loss: 1.492592  [ 6432/60000]\n",
            "loss: 1.462073  [ 9632/60000]\n",
            "loss: 1.514330  [12832/60000]\n",
            "loss: 1.464590  [16032/60000]\n",
            "loss: 1.492558  [19232/60000]\n",
            "loss: 1.480279  [22432/60000]\n",
            "loss: 1.486926  [25632/60000]\n",
            "loss: 1.510150  [28832/60000]\n",
            "loss: 1.461228  [32032/60000]\n",
            "loss: 1.470008  [35232/60000]\n",
            "loss: 1.505543  [38432/60000]\n",
            "loss: 1.461289  [41632/60000]\n",
            "loss: 1.523963  [44832/60000]\n",
            "loss: 1.465701  [48032/60000]\n",
            "loss: 1.561995  [51232/60000]\n",
            "loss: 1.492546  [54432/60000]\n",
            "loss: 1.492433  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.2%, Avg loss: 1.489439 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.491808  [   32/60000]\n",
            "loss: 1.473433  [ 3232/60000]\n",
            "loss: 1.501799  [ 6432/60000]\n",
            "loss: 1.546365  [ 9632/60000]\n",
            "loss: 1.523078  [12832/60000]\n",
            "loss: 1.461269  [16032/60000]\n",
            "loss: 1.462270  [19232/60000]\n",
            "loss: 1.524438  [22432/60000]\n",
            "loss: 1.461189  [25632/60000]\n",
            "loss: 1.461193  [28832/60000]\n",
            "loss: 1.524876  [32032/60000]\n",
            "loss: 1.461218  [35232/60000]\n",
            "loss: 1.461322  [38432/60000]\n",
            "loss: 1.466197  [41632/60000]\n",
            "loss: 1.482059  [44832/60000]\n",
            "loss: 1.493480  [48032/60000]\n",
            "loss: 1.461282  [51232/60000]\n",
            "loss: 1.523373  [54432/60000]\n",
            "loss: 1.461963  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.8%, Avg loss: 1.482612 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.481559  [   32/60000]\n",
            "loss: 1.492244  [ 3232/60000]\n",
            "loss: 1.520859  [ 6432/60000]\n",
            "loss: 1.522968  [ 9632/60000]\n",
            "loss: 1.461261  [12832/60000]\n",
            "loss: 1.461280  [16032/60000]\n",
            "loss: 1.461177  [19232/60000]\n",
            "loss: 1.523015  [22432/60000]\n",
            "loss: 1.525633  [25632/60000]\n",
            "loss: 1.461972  [28832/60000]\n",
            "loss: 1.461202  [32032/60000]\n",
            "loss: 1.461164  [35232/60000]\n",
            "loss: 1.555009  [38432/60000]\n",
            "loss: 1.486399  [41632/60000]\n",
            "loss: 1.492369  [44832/60000]\n",
            "loss: 1.463513  [48032/60000]\n",
            "loss: 1.493191  [51232/60000]\n",
            "loss: 1.492387  [54432/60000]\n",
            "loss: 1.461252  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.3%, Avg loss: 1.488173 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYDLWjdseB4H"
      },
      "source": [
        "### Zadanie 2\n",
        "Wykorzystując model z przykładu, napisz sieć rekurencyjną przy użyciu RNNCell.\n",
        "\n",
        "RNNCell implementuje tylko operacje wykonywane przez warstwę\n",
        "rekurencyjną dla jednego kroku. Warstwy rekurencyjne w każdym kroku\n",
        "łączą wynik operacji poprzedniego kroku i aktualny input.\n",
        "Wykorzystaj pętle for do wielokrotnego wywołania komórki RNNCell (liczba kroków to liczba elementów w sekwencji).\n",
        "\n",
        "Wywołanie zainicjalizowanej komórki rekurencyjnej wymaga podania aktualnego inputu i listy stanów ukrytych poprzedniego kroku (RNNCell ma jeden stan).\n",
        "\n",
        "Trzeba zainicjalizować ukryty stan warstwy z wartościami początkowymi (można wykorzystać zmienne losowe - torch.rand)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UXwkHVXLfEky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19b019b8-7e10-4a6a-abdd-f88ead64f292"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RecurrentModel3(\n",
              "  (rnn_cell): RNNCell(28, 128)\n",
              "  (dense): Linear(in_features=128, out_features=10, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RecurrentModel3(nn.Module):\n",
        "    def __init__(self, input_size=28, hidden_size=128, num_classes=10):\n",
        "        super(RecurrentModel3, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Define the RNN cell\n",
        "        self.rnn_cell = nn.RNNCell(input_size=input_size, hidden_size=hidden_size)\n",
        "        self.dense = nn.Linear(hidden_size, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if inputs.dim() == 4:\n",
        "            # Example: (batch_size, channels, sequence_length, features)\n",
        "            inputs = inputs.squeeze(1)  # Remove the channels dimension if it's 1\n",
        "        elif inputs.dim() != 3:\n",
        "            raise ValueError(f\"Expected input to be 3D, got {inputs.dim()}D instead.\")\n",
        "\n",
        "        batch_size, sequence_length, input_size = inputs.size()\n",
        "        # TODO forward\n",
        "        hidden_state = torch.rand(batch_size, self.hidden_size, device=inputs.device)\n",
        "\n",
        "        for t in range(sequence_length):\n",
        "            current_input = inputs[:, t, :]\n",
        "            hidden_state = self.rnn_cell(current_input, hidden_state)\n",
        "\n",
        "        output = self.dense(hidden_state)\n",
        "        return self.softmax(output)\n",
        "\n",
        "model = RecurrentModel3(input_size=28, hidden_size=128, num_classes=10)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CVmL0U34fEky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da7c4ed5-005c-4cec-bd37-a1e78bf9bdc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.303405  [   32/60000]\n",
            "loss: 2.042854  [ 3232/60000]\n",
            "loss: 2.041989  [ 6432/60000]\n",
            "loss: 1.976282  [ 9632/60000]\n",
            "loss: 1.920557  [12832/60000]\n",
            "loss: 1.778276  [16032/60000]\n",
            "loss: 1.990311  [19232/60000]\n",
            "loss: 1.817736  [22432/60000]\n",
            "loss: 1.771817  [25632/60000]\n",
            "loss: 1.879548  [28832/60000]\n",
            "loss: 1.831595  [32032/60000]\n",
            "loss: 1.800080  [35232/60000]\n",
            "loss: 1.669052  [38432/60000]\n",
            "loss: 1.848550  [41632/60000]\n",
            "loss: 1.908566  [44832/60000]\n",
            "loss: 1.893922  [48032/60000]\n",
            "loss: 1.835011  [51232/60000]\n",
            "loss: 1.735883  [54432/60000]\n",
            "loss: 1.749113  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 73.9%, Avg loss: 1.730358 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.716027  [   32/60000]\n",
            "loss: 1.752628  [ 3232/60000]\n",
            "loss: 1.782749  [ 6432/60000]\n",
            "loss: 1.804466  [ 9632/60000]\n",
            "loss: 1.789232  [12832/60000]\n",
            "loss: 1.805423  [16032/60000]\n",
            "loss: 1.717397  [19232/60000]\n",
            "loss: 1.720725  [22432/60000]\n",
            "loss: 1.557810  [25632/60000]\n",
            "loss: 1.620712  [28832/60000]\n",
            "loss: 1.637956  [32032/60000]\n",
            "loss: 1.692445  [35232/60000]\n",
            "loss: 1.658002  [38432/60000]\n",
            "loss: 1.684222  [41632/60000]\n",
            "loss: 1.650051  [44832/60000]\n",
            "loss: 1.580343  [48032/60000]\n",
            "loss: 1.743940  [51232/60000]\n",
            "loss: 1.620445  [54432/60000]\n",
            "loss: 1.663682  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.3%, Avg loss: 1.624954 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.645639  [   32/60000]\n",
            "loss: 1.613683  [ 3232/60000]\n",
            "loss: 1.559381  [ 6432/60000]\n",
            "loss: 1.699658  [ 9632/60000]\n",
            "loss: 1.741012  [12832/60000]\n",
            "loss: 1.576806  [16032/60000]\n",
            "loss: 1.558072  [19232/60000]\n",
            "loss: 1.533922  [22432/60000]\n",
            "loss: 1.683307  [25632/60000]\n",
            "loss: 1.534660  [28832/60000]\n",
            "loss: 1.677619  [32032/60000]\n",
            "loss: 1.591020  [35232/60000]\n",
            "loss: 1.597212  [38432/60000]\n",
            "loss: 1.627437  [41632/60000]\n",
            "loss: 1.699050  [44832/60000]\n",
            "loss: 1.628577  [48032/60000]\n",
            "loss: 1.709516  [51232/60000]\n",
            "loss: 1.563933  [54432/60000]\n",
            "loss: 1.623891  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.3%, Avg loss: 1.612611 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.662129  [   32/60000]\n",
            "loss: 1.600056  [ 3232/60000]\n",
            "loss: 1.613026  [ 6432/60000]\n",
            "loss: 1.698792  [ 9632/60000]\n",
            "loss: 1.694377  [12832/60000]\n",
            "loss: 1.658792  [16032/60000]\n",
            "loss: 1.524991  [19232/60000]\n",
            "loss: 1.555092  [22432/60000]\n",
            "loss: 1.540329  [25632/60000]\n",
            "loss: 1.609487  [28832/60000]\n",
            "loss: 1.539965  [32032/60000]\n",
            "loss: 1.552539  [35232/60000]\n",
            "loss: 1.590886  [38432/60000]\n",
            "loss: 1.617941  [41632/60000]\n",
            "loss: 1.526045  [44832/60000]\n",
            "loss: 1.527959  [48032/60000]\n",
            "loss: 1.573661  [51232/60000]\n",
            "loss: 1.586819  [54432/60000]\n",
            "loss: 1.673673  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.4%, Avg loss: 1.577562 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.575082  [   32/60000]\n",
            "loss: 1.507908  [ 3232/60000]\n",
            "loss: 1.708541  [ 6432/60000]\n",
            "loss: 1.618454  [ 9632/60000]\n",
            "loss: 1.570715  [12832/60000]\n",
            "loss: 1.591254  [16032/60000]\n",
            "loss: 1.863085  [19232/60000]\n",
            "loss: 1.640946  [22432/60000]\n",
            "loss: 1.603847  [25632/60000]\n",
            "loss: 1.835768  [28832/60000]\n",
            "loss: 1.627802  [32032/60000]\n",
            "loss: 1.504821  [35232/60000]\n",
            "loss: 1.524291  [38432/60000]\n",
            "loss: 1.613516  [41632/60000]\n",
            "loss: 1.645803  [44832/60000]\n",
            "loss: 1.618350  [48032/60000]\n",
            "loss: 1.578709  [51232/60000]\n",
            "loss: 1.679366  [54432/60000]\n",
            "loss: 1.589432  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.1%, Avg loss: 1.571243 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyPGkC6oiEd5"
      },
      "source": [
        "### Zadanie 3\n",
        "Zamień komórkę rekurencyjną z poprzedniego zadania na LSTMCell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "C5MPQ1UcigN5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31981add-0b1e-4a4b-b3b3-0016a4eb71f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RecurrentModel4(\n",
              "  (rnn_cell): LSTMCell(28, 128)\n",
              "  (dense): Linear(in_features=128, out_features=10, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RecurrentModel4(nn.Module):\n",
        "    def __init__(self, input_size=28, hidden_size=128, num_classes=10):\n",
        "        super(RecurrentModel4, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Define the RNN -> LSTM cell\n",
        "        self.rnn_cell = nn.LSTMCell(input_size=input_size, hidden_size=hidden_size)\n",
        "        self.dense = nn.Linear(hidden_size, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if inputs.dim() == 4:\n",
        "            # Example: (batch_size, channels, sequence_length, features)\n",
        "            inputs = inputs.squeeze(1)  # Remove the channels dimension if it's 1\n",
        "        elif inputs.dim() != 3:\n",
        "            raise ValueError(f\"Expected input to be 3D, got {inputs.dim()}D instead.\")\n",
        "\n",
        "        batch_size, sequence_length, input_size = inputs.size()\n",
        "\n",
        "        # Initialize hidden state and cell state with random values\n",
        "        hidden_state = torch.rand(batch_size, self.hidden_size, device=inputs.device)\n",
        "        cell_state = torch.rand(batch_size, self.hidden_size, device=inputs.device)\n",
        "\n",
        "        for t in range(sequence_length):\n",
        "            current_input = inputs[:, t, :]\n",
        "            hidden_state, cell_state = self.rnn_cell(current_input, (hidden_state, cell_state))\n",
        "\n",
        "        output = self.dense(hidden_state)\n",
        "        return self.softmax(output)\n",
        "\n",
        "model = RecurrentModel4(input_size=28, hidden_size=128, num_classes=10)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8cxbdnUNfEkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "c6053bce-af85-4a44-c457-ba21bfdb9a72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.302319  [   32/60000]\n",
            "loss: 2.031943  [ 3232/60000]\n",
            "loss: 1.802835  [ 6432/60000]\n",
            "loss: 1.580184  [ 9632/60000]\n",
            "loss: 1.676719  [12832/60000]\n",
            "loss: 1.620912  [16032/60000]\n",
            "loss: 1.589483  [19232/60000]\n",
            "loss: 1.635175  [22432/60000]\n",
            "loss: 1.609765  [25632/60000]\n",
            "loss: 1.545296  [28832/60000]\n",
            "loss: 1.610026  [32032/60000]\n",
            "loss: 1.499369  [35232/60000]\n",
            "loss: 1.542769  [38432/60000]\n",
            "loss: 1.540112  [41632/60000]\n",
            "loss: 1.532936  [44832/60000]\n",
            "loss: 1.547606  [48032/60000]\n",
            "loss: 1.559061  [51232/60000]\n",
            "loss: 1.496145  [54432/60000]\n",
            "loss: 1.562528  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 94.3%, Avg loss: 1.519935 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.553204  [   32/60000]\n",
            "loss: 1.547702  [ 3232/60000]\n",
            "loss: 1.593671  [ 6432/60000]\n",
            "loss: 1.500484  [ 9632/60000]\n",
            "loss: 1.517636  [12832/60000]\n",
            "loss: 1.557179  [16032/60000]\n",
            "loss: 1.499529  [19232/60000]\n",
            "loss: 1.578616  [22432/60000]\n",
            "loss: 1.512796  [25632/60000]\n",
            "loss: 1.463974  [28832/60000]\n",
            "loss: 1.566862  [32032/60000]\n",
            "loss: 1.463969  [35232/60000]\n",
            "loss: 1.541829  [38432/60000]\n",
            "loss: 1.530640  [41632/60000]\n",
            "loss: 1.571179  [44832/60000]\n",
            "loss: 1.466980  [48032/60000]\n",
            "loss: 1.492572  [51232/60000]\n",
            "loss: 1.461623  [54432/60000]\n",
            "loss: 1.577205  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.6%, Avg loss: 1.506869 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.492563  [   32/60000]\n",
            "loss: 1.490716  [ 3232/60000]\n",
            "loss: 1.461995  [ 6432/60000]\n",
            "loss: 1.537850  [ 9632/60000]\n",
            "loss: 1.499633  [12832/60000]\n",
            "loss: 1.523732  [16032/60000]\n",
            "loss: 1.493656  [19232/60000]\n",
            "loss: 1.461312  [22432/60000]\n",
            "loss: 1.494304  [25632/60000]\n",
            "loss: 1.477351  [28832/60000]\n",
            "loss: 1.462784  [32032/60000]\n",
            "loss: 1.492696  [35232/60000]\n",
            "loss: 1.466160  [38432/60000]\n",
            "loss: 1.492708  [41632/60000]\n",
            "loss: 1.585294  [44832/60000]\n",
            "loss: 1.465289  [48032/60000]\n",
            "loss: 1.503312  [51232/60000]\n",
            "loss: 1.493061  [54432/60000]\n",
            "loss: 1.490301  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.7%, Avg loss: 1.494157 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.470538  [   32/60000]\n",
            "loss: 1.524184  [ 3232/60000]\n",
            "loss: 1.500420  [ 6432/60000]\n",
            "loss: 1.526572  [ 9632/60000]\n",
            "loss: 1.491161  [12832/60000]\n",
            "loss: 1.523816  [16032/60000]\n",
            "loss: 1.492443  [19232/60000]\n",
            "loss: 1.462305  [22432/60000]\n",
            "loss: 1.498059  [25632/60000]\n",
            "loss: 1.484335  [28832/60000]\n",
            "loss: 1.495311  [32032/60000]\n",
            "loss: 1.484536  [35232/60000]\n",
            "loss: 1.496136  [38432/60000]\n",
            "loss: 1.476067  [41632/60000]\n",
            "loss: 1.492818  [44832/60000]\n",
            "loss: 1.486384  [48032/60000]\n",
            "loss: 1.462515  [51232/60000]\n",
            "loss: 1.461256  [54432/60000]\n",
            "loss: 1.492112  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.8%, Avg loss: 1.494355 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.522046  [   32/60000]\n",
            "loss: 1.461593  [ 3232/60000]\n",
            "loss: 1.516769  [ 6432/60000]\n",
            "loss: 1.522881  [ 9632/60000]\n",
            "loss: 1.554726  [12832/60000]\n",
            "loss: 1.556093  [16032/60000]\n",
            "loss: 1.499783  [19232/60000]\n",
            "loss: 1.461496  [22432/60000]\n",
            "loss: 1.522139  [25632/60000]\n",
            "loss: 1.461806  [28832/60000]\n",
            "loss: 1.492470  [32032/60000]\n",
            "loss: 1.513763  [35232/60000]\n",
            "loss: 1.461223  [38432/60000]\n",
            "loss: 1.550925  [41632/60000]\n",
            "loss: 1.472600  [44832/60000]\n",
            "loss: 1.523718  [48032/60000]\n",
            "loss: 1.492532  [51232/60000]\n",
            "loss: 1.527225  [54432/60000]\n",
            "loss: 1.462465  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.7%, Avg loss: 1.493701 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prwjaEv2efs3"
      },
      "source": [
        "### Zadanie 4\n",
        "Wykorzystując model z poprzedniego zadania, stwórz model sieci\n",
        "neuronowej z własną implementacją prostej warstwy rekurencyjnej.\n",
        "- w call zamień self.lstm_cell_layer(x) na wyołanie własnej metody np. self.cell(x)\n",
        "- w konstruktorze modelu usuń inicjalizację komórki LSTM i zastąp ją inicjalizacją warstw potrzebnych do stworzenia własnej komórki rekurencyjnej,\n",
        "- stwórz metodę cell() wykonującą operacje warstwy rekurencyjnej,\n",
        "- prosta warstwa rekurencyjna konkatenuje poprzedni wyniki i aktualny input, a następnie przepuszcza ten połączony tensor przez warstwę gęstą (Dense)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tn5PgdhHfEkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e7a3a03-630e-4660-f6ac-06e1cc28b8bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RecurrentModel5(\n",
              "  (input_dense): Linear(in_features=156, out_features=128, bias=True)\n",
              "  (activation): Tanh()\n",
              "  (output_dense): Linear(in_features=128, out_features=10, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "class RecurrentModel5(nn.Module):\n",
        "    def __init__(self, input_size=28, hidden_size=128, num_classes=10):\n",
        "        super(RecurrentModel5, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.input_dense = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "        self.output_dense = nn.Linear(hidden_size, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def cell(self, current_input, hidden_state):\n",
        "        combined = torch.cat((current_input, hidden_state), dim=1)\n",
        "        new_hidden_state = self.activation(self.input_dense(combined))\n",
        "        return new_hidden_state\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if inputs.dim() == 4:\n",
        "            # Example: (batch_size, channels, sequence_length, features)\n",
        "            inputs = inputs.squeeze(1)  # Remove the channels dimension if it's 1\n",
        "        elif inputs.dim() != 3:\n",
        "            raise ValueError(f\"Expected input to be 3D, got {inputs.dim()}D instead.\")\n",
        "\n",
        "        batch_size, sequence_length, input_size = inputs.size()\n",
        "        # TODO forward\n",
        "        hidden_state = torch.rand(batch_size, self.hidden_size, device=inputs.device)\n",
        "\n",
        "        for t in range(sequence_length):\n",
        "            current_input = inputs[:, t, :]\n",
        "            hidden_state = self.cell(current_input, hidden_state)\n",
        "\n",
        "        output = self.output_dense(hidden_state)\n",
        "        return self.softmax(output)\n",
        "\n",
        "model = RecurrentModel5(input_size=28, hidden_size=128, num_classes=10)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uNNkN9LGfEkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b19ee57-a342-4e13-b047-f122a43d605b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.302666  [   32/60000]\n",
            "loss: 2.281349  [ 3232/60000]\n",
            "loss: 2.288476  [ 6432/60000]\n",
            "loss: 2.324862  [ 9632/60000]\n",
            "loss: 2.086435  [12832/60000]\n",
            "loss: 2.066480  [16032/60000]\n",
            "loss: 2.033692  [19232/60000]\n",
            "loss: 1.926751  [22432/60000]\n",
            "loss: 2.014091  [25632/60000]\n",
            "loss: 1.939826  [28832/60000]\n",
            "loss: 1.939030  [32032/60000]\n",
            "loss: 2.028589  [35232/60000]\n",
            "loss: 1.956050  [38432/60000]\n",
            "loss: 2.121966  [41632/60000]\n",
            "loss: 1.825640  [44832/60000]\n",
            "loss: 1.834677  [48032/60000]\n",
            "loss: 1.880324  [51232/60000]\n",
            "loss: 1.808635  [54432/60000]\n",
            "loss: 2.132646  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.2%, Avg loss: 1.842621 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.765394  [   32/60000]\n",
            "loss: 1.929018  [ 3232/60000]\n",
            "loss: 1.702716  [ 6432/60000]\n",
            "loss: 1.862526  [ 9632/60000]\n",
            "loss: 1.918984  [12832/60000]\n",
            "loss: 1.894815  [16032/60000]\n",
            "loss: 1.813027  [19232/60000]\n",
            "loss: 1.844542  [22432/60000]\n",
            "loss: 1.745414  [25632/60000]\n",
            "loss: 1.901624  [28832/60000]\n",
            "loss: 1.799667  [32032/60000]\n",
            "loss: 1.633257  [35232/60000]\n",
            "loss: 1.945658  [38432/60000]\n",
            "loss: 1.714284  [41632/60000]\n",
            "loss: 1.803690  [44832/60000]\n",
            "loss: 1.764795  [48032/60000]\n",
            "loss: 2.040697  [51232/60000]\n",
            "loss: 1.790659  [54432/60000]\n",
            "loss: 1.847465  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 69.0%, Avg loss: 1.773853 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.785670  [   32/60000]\n",
            "loss: 1.793008  [ 3232/60000]\n",
            "loss: 1.744673  [ 6432/60000]\n",
            "loss: 1.836798  [ 9632/60000]\n",
            "loss: 1.611314  [12832/60000]\n",
            "loss: 1.719965  [16032/60000]\n",
            "loss: 1.664566  [19232/60000]\n",
            "loss: 1.692652  [22432/60000]\n",
            "loss: 1.735280  [25632/60000]\n",
            "loss: 1.810234  [28832/60000]\n",
            "loss: 1.844066  [32032/60000]\n",
            "loss: 1.540687  [35232/60000]\n",
            "loss: 1.758976  [38432/60000]\n",
            "loss: 1.655016  [41632/60000]\n",
            "loss: 1.709398  [44832/60000]\n",
            "loss: 1.661360  [48032/60000]\n",
            "loss: 1.685287  [51232/60000]\n",
            "loss: 1.667405  [54432/60000]\n",
            "loss: 1.662333  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 74.2%, Avg loss: 1.720355 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.636028  [   32/60000]\n",
            "loss: 1.609943  [ 3232/60000]\n",
            "loss: 1.710833  [ 6432/60000]\n",
            "loss: 1.737800  [ 9632/60000]\n",
            "loss: 1.731495  [12832/60000]\n",
            "loss: 1.642934  [16032/60000]\n",
            "loss: 1.673101  [19232/60000]\n",
            "loss: 1.676279  [22432/60000]\n",
            "loss: 1.657559  [25632/60000]\n",
            "loss: 1.679954  [28832/60000]\n",
            "loss: 1.732662  [32032/60000]\n",
            "loss: 1.749172  [35232/60000]\n",
            "loss: 1.835467  [38432/60000]\n",
            "loss: 1.684149  [41632/60000]\n",
            "loss: 1.657755  [44832/60000]\n",
            "loss: 1.660105  [48032/60000]\n",
            "loss: 1.679062  [51232/60000]\n",
            "loss: 1.625018  [54432/60000]\n",
            "loss: 1.823411  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.3%, Avg loss: 1.638889 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.719100  [   32/60000]\n",
            "loss: 1.582840  [ 3232/60000]\n",
            "loss: 1.599794  [ 6432/60000]\n",
            "loss: 1.588554  [ 9632/60000]\n",
            "loss: 1.695521  [12832/60000]\n",
            "loss: 1.682022  [16032/60000]\n",
            "loss: 1.658348  [19232/60000]\n",
            "loss: 1.641075  [22432/60000]\n",
            "loss: 1.610992  [25632/60000]\n",
            "loss: 1.708126  [28832/60000]\n",
            "loss: 1.862346  [32032/60000]\n",
            "loss: 1.586952  [35232/60000]\n",
            "loss: 1.513646  [38432/60000]\n",
            "loss: 1.606851  [41632/60000]\n",
            "loss: 1.676894  [44832/60000]\n",
            "loss: 1.663119  [48032/60000]\n",
            "loss: 1.708872  [51232/60000]\n",
            "loss: 1.712271  [54432/60000]\n",
            "loss: 1.676945  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.6%, Avg loss: 1.615353 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 5\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3sOaUu3b77l"
      },
      "source": [
        "### Zadanie 5\n",
        "\n",
        "Na podstawie modelu z poprzedniego zadania stwórz model z własną implementacją warstwy LSTM. Dokładny i zrozumiały opis działania wartswy LSTM znajduje się na [stronie](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "hkCPXSXnfEk0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88d321f9-10f8-4d49-9347-c0a040c4f3e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RecurrentModel6(\n",
              "  (input_dense): Linear(in_features=156, out_features=128, bias=True)\n",
              "  (activation): Tanh()\n",
              "  (output_dense): Linear(in_features=128, out_features=10, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "from torch.nn.modules.activation import Sigmoid, Tanh\n",
        "\n",
        "class RecurrentModel6(nn.Module):\n",
        "    def __init__(self, input_size=28, hidden_size=128, num_classes=10):\n",
        "        super(RecurrentModel6, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Define LSTM layers\n",
        "        self.input_dense = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "        self.output_dense = nn.Linear(hidden_size, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def cell(self, current_input, hidden_state):\n",
        "        combined = torch.cat((current_input, hidden_state), dim=1)\n",
        "        new_hidden_state = self.activation(self.input_dense(combined))\n",
        "        return new_hidden_state\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if inputs.dim() == 4:\n",
        "            # Example: (batch_size, channels, sequence_length, features)\n",
        "            inputs = inputs.squeeze(1)  # Remove the channels dimension if it's 1\n",
        "        elif inputs.dim() != 3:\n",
        "            raise ValueError(f\"Expected input to be 3D, got {inputs.dim()}D instead.\")\n",
        "\n",
        "        batch_size, sequence_length, input_size = inputs.size()\n",
        "        # TODO forward\n",
        "        hidden_state = torch.rand(batch_size, self.hidden_size, device=inputs.device)\n",
        "\n",
        "        for t in range(sequence_length):\n",
        "            current_input = inputs[:, t, :]\n",
        "            hidden_state = self.cell(current_input, hidden_state)\n",
        "\n",
        "        output = self.output_dense(hidden_state)\n",
        "        return self.softmax(output)\n",
        "\n",
        "model = RecurrentModel6(input_size=28, hidden_size=128, num_classes=10)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "BQEm8GHqfEk0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d14d8ebe-8082-4b00-9143-2277e77af9eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.034476  [   32/60000]\n",
            "loss: 2.127038  [ 3232/60000]\n",
            "loss: 1.972545  [ 6432/60000]\n",
            "loss: 2.008631  [ 9632/60000]\n",
            "loss: 1.939014  [12832/60000]\n",
            "loss: 1.806337  [16032/60000]\n",
            "loss: 1.938730  [19232/60000]\n",
            "loss: 1.752054  [22432/60000]\n",
            "loss: 1.537331  [25632/60000]\n",
            "loss: 1.775865  [28832/60000]\n",
            "loss: 1.776768  [32032/60000]\n",
            "loss: 1.783341  [35232/60000]\n",
            "loss: 1.834061  [38432/60000]\n",
            "loss: 1.649384  [41632/60000]\n",
            "loss: 1.864385  [44832/60000]\n",
            "loss: 1.718671  [48032/60000]\n",
            "loss: 1.667906  [51232/60000]\n",
            "loss: 2.151249  [54432/60000]\n",
            "loss: 1.727632  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 76.4%, Avg loss: 1.702773 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.835730  [   32/60000]\n",
            "loss: 1.745147  [ 3232/60000]\n",
            "loss: 1.605351  [ 6432/60000]\n",
            "loss: 1.805675  [ 9632/60000]\n",
            "loss: 1.707496  [12832/60000]\n",
            "loss: 1.668104  [16032/60000]\n",
            "loss: 1.727616  [19232/60000]\n",
            "loss: 1.745713  [22432/60000]\n",
            "loss: 1.739569  [25632/60000]\n",
            "loss: 1.718007  [28832/60000]\n",
            "loss: 1.639531  [32032/60000]\n",
            "loss: 1.687397  [35232/60000]\n",
            "loss: 1.673941  [38432/60000]\n",
            "loss: 1.600798  [41632/60000]\n",
            "loss: 1.686846  [44832/60000]\n",
            "loss: 1.647213  [48032/60000]\n",
            "loss: 1.530442  [51232/60000]\n",
            "loss: 1.587692  [54432/60000]\n",
            "loss: 1.576779  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.0%, Avg loss: 1.602855 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.719862  [   32/60000]\n",
            "loss: 1.641379  [ 3232/60000]\n",
            "loss: 1.525200  [ 6432/60000]\n",
            "loss: 1.553351  [ 9632/60000]\n",
            "loss: 1.649541  [12832/60000]\n",
            "loss: 1.665220  [16032/60000]\n",
            "loss: 1.624215  [19232/60000]\n",
            "loss: 1.618483  [22432/60000]\n",
            "loss: 1.553137  [25632/60000]\n",
            "loss: 1.535761  [28832/60000]\n",
            "loss: 1.615764  [32032/60000]\n",
            "loss: 1.527700  [35232/60000]\n",
            "loss: 1.523744  [38432/60000]\n",
            "loss: 1.677161  [41632/60000]\n",
            "loss: 1.635094  [44832/60000]\n",
            "loss: 1.584696  [48032/60000]\n",
            "loss: 1.505779  [51232/60000]\n",
            "loss: 1.816481  [54432/60000]\n",
            "loss: 1.540037  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.3%, Avg loss: 1.601048 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.541825  [   32/60000]\n",
            "loss: 1.608789  [ 3232/60000]\n",
            "loss: 1.526092  [ 6432/60000]\n",
            "loss: 1.557886  [ 9632/60000]\n",
            "loss: 1.510358  [12832/60000]\n",
            "loss: 1.600949  [16032/60000]\n",
            "loss: 1.665078  [19232/60000]\n",
            "loss: 1.594296  [22432/60000]\n",
            "loss: 1.656006  [25632/60000]\n",
            "loss: 1.618604  [28832/60000]\n",
            "loss: 1.517130  [32032/60000]\n",
            "loss: 1.654015  [35232/60000]\n",
            "loss: 1.569592  [38432/60000]\n",
            "loss: 1.537473  [41632/60000]\n",
            "loss: 1.611070  [44832/60000]\n",
            "loss: 1.553624  [48032/60000]\n",
            "loss: 1.652683  [51232/60000]\n",
            "loss: 1.491429  [54432/60000]\n",
            "loss: 1.645893  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 90.2%, Avg loss: 1.560519 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.497531  [   32/60000]\n",
            "loss: 1.589548  [ 3232/60000]\n",
            "loss: 1.598410  [ 6432/60000]\n",
            "loss: 1.600193  [ 9632/60000]\n",
            "loss: 1.562353  [12832/60000]\n",
            "loss: 1.637880  [16032/60000]\n",
            "loss: 1.533503  [19232/60000]\n",
            "loss: 1.588710  [22432/60000]\n",
            "loss: 1.617837  [25632/60000]\n",
            "loss: 1.556402  [28832/60000]\n",
            "loss: 1.553068  [32032/60000]\n",
            "loss: 1.504653  [35232/60000]\n",
            "loss: 1.549471  [38432/60000]\n",
            "loss: 1.645575  [41632/60000]\n",
            "loss: 1.524011  [44832/60000]\n",
            "loss: 1.558040  [48032/60000]\n",
            "loss: 1.513203  [51232/60000]\n",
            "loss: 1.561955  [54432/60000]\n",
            "loss: 1.535946  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.6%, Avg loss: 1.566671 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 5\n",
        "learning_rate = 0.001\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv-snum",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}