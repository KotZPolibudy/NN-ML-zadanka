{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Attribute selection in classification problems\n",
    "\n",
    "### Curse of dimensionality\n",
    "\n",
    "**Question** What is \"curse of dimensionality\"? What does it mean in practice?\n",
    "\n",
    "**Question** How can we reduce the number of attributes in classification problems?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download file seizure.csv containing information about the recording of brain activity of people (description of the dataset: https://archive.ics.uci.edu/dataset/388/epileptic+seizure+recognition).\n",
    "Each row contains 178 subsequent EEG measurements conducted during 1 second (column), the last column represents the label y {1,2,3,4,5} (subjects in class 1 have epileptic seizure, other classes represent different control groups, such as eyes open (5), or eyes closed (4)).\n",
    "\n",
    "The train and test split and normalization is already done. \n",
    "Your task is to classify the given data with k-NN with k=3 (as during previous laboratories) and measure the accuracy.\n",
    "\n",
    "Next, try to eliminate redundant attributes. Reduce the number of attributes to 30. Use the SelectKBest class. Remember to fit it only on train data. To reduce the test data to obtain only remained parameters use transform method on fit SelectKBest object. Use mutual_info_classif measure to assess the relevance of attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "data = pd.read_csv(\"seizure.csv\")\n",
    "data_Y = data[\"y\"]\n",
    "data = data.drop(columns=[\"Name\", \"y\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, data_Y, test_size=0.33, random_state=42)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO you are given data in X_train, X_test, y_train, y_test variables\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X_train, y_train)\n",
    "acc = accuracy_score(y_test, neigh.predict(X_test))\n",
    "print(acc)\n",
    "\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=30)\n",
    "X_train_reduced = selector.fit_transform(X_train, y_train)\n",
    "X_test_reduced = selector.transform(X_test)\n",
    "\n",
    "neigh2 = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh2.fit(X_train_reduced, y_train)\n",
    "acc = accuracy_score(y_test, neigh2.predict(X_test_reduced))\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the exercise above we set the number of features arbitrarily. \n",
    "\n",
    "**Question** How can we find the optimal number of attributes in machine learning problems?\n",
    "\n",
    "*moja odpowiedź*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clustering\n",
    "\n",
    "**Question** What is clustering? What can we achieve with clustering?\n",
    "\n",
    "**Question** What clustering algorithms do you know?\n",
    "\n",
    "*moja odpowiedź*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. K-means algorithm\n",
    "\n",
    "**Question** What are the following steps of the k-means algorithm?\n",
    "\n",
    "**Question** How can we choose the initial clusters?\n",
    "\n",
    "*moja odpowiedź*\n",
    "\n",
    "1. Inicjalizacja centroid\n",
    "2. Przypisanie danych do najbliższej centroidy\n",
    "3. Przeliczenie nowych centroid (średnia punktów danego klastra)\n",
    "4. Powtarzać 2-3 aż do określonego warunku końca (np. zmiany mniejszej od zadanego progu, lub ilości iteracji)\n",
    "\n",
    "\n",
    "Jak wybierać?\n",
    "- wiedza ekspercka (narzucone z góry dla algorytmu)\n",
    "- losowo (albo punkt z przestrzeni, albo losowa dana\n",
    "- wybrać widoczne odstające przypadki (outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "Given the following examples of grades od 5 students we want to divide them into 2 groups:\n",
    "\n",
    "| Subject | A   | B   |\n",
    "|---------|-----|-----|\n",
    "| 1       | 1.0 | 1.0 |\n",
    "| 2       | 1.5 | 2.0 |\n",
    "| 3       | 3.0 | 3.0 |\n",
    "| 4       | 5.0 | 7.0 |\n",
    "| 5       | 3.5 | 5.0 |\n",
    "\n",
    "We have chosen the two furthest students (using euclidean distance) as the initial clusters' centroids:\n",
    "\n",
    "|Cluster|Centroid|A  |B  |\n",
    "|-------|--------|---|---|\n",
    "|C1     |k1      |1.0|1.0|\n",
    "|C2     |k2      |5.0|7.0|\n",
    "\n",
    "Perform the first iteration of k-means: divide all students into clusters and find the centroids of these clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.118033988749895 6.103277807866851\n",
      "2.8284271247461903 4.47213595499958\n",
      "4.716990566028302 2.5\n",
      "(1.8333333333333333, 2.0)\n",
      "(4.25, 6.0)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def dist(p1, p2):\n",
    "    return math.sqrt((p2[0] - p1[0]) ** 2 + (p2[1] - p1[1]) ** 2)\n",
    "\n",
    "def mean_point(plist):\n",
    "    sum_x = sum(point[0] for point in plist)\n",
    "    sum_y = sum(point[1] for point in plist)\n",
    "    mean_x = sum_x / len(plist)\n",
    "    mean_y = sum_y / len(plist)\n",
    "    return mean_x, mean_y\n",
    "\n",
    "p1 = (1.0, 1.0)\n",
    "p2 = (1.5, 2.0)\n",
    "p3 = (3.0, 3.0)\n",
    "p4 = (5.0, 7.0)\n",
    "p5 = (3.5, 5.0)\n",
    "\n",
    "d21 = dist(p2, p1)\n",
    "d31 = dist(p3, p1)\n",
    "d51 = dist(p5, p1)\n",
    "\n",
    "d24 = dist(p2, p4)\n",
    "d34 = dist(p3, p4)\n",
    "d54 = dist(p5, p4)\n",
    "\n",
    "print(d21, d24)\n",
    "print(d31, d34)  # punkt 3 należy do klastra C2\n",
    "print(d51, d54)  # punkt 5 należy do klastra C2\n",
    "\n",
    "# C1:  S1, S2, S3\n",
    "# C2: S4, S5\n",
    "\n",
    "# nowe centroidy\n",
    "c1 = mean_point([p1, p2, p3])\n",
    "print(c1) #(1.83, 2.0)\n",
    "c2 = mean_point([p4, p5])\n",
    "print(c2) #(4.25, 6.0)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** When the algorithm should stop?\n",
    "\n",
    "**Question** What advantages and disadvantages of k-means clustering can you find?\n",
    "\n",
    "\n",
    "*moja odpowiedź*\n",
    "\n",
    "Jak w opisie wyżej, np. po określonej ilości iteracji, lub gdy zmiany centroid nie przynoszą znaczących efektów (np. centroida przesuwa się o marginalnie małe wartości, lub nie zmieniają się elementy należące do danego klastra)\n",
    "\n",
    "Zalety:\n",
    "+ Prostota (a przez to wydajność) rozwiązania\n",
    "+ Skalowalność (dodanie nawet dużej ilości danych nie jest problemem\n",
    "+ popularność\n",
    "\n",
    "Wady:\n",
    "- Czułość na wartości inicjalizujące (pierwsze centroidy i ilość klastrów)\n",
    "- założenie kolistych klastrów\n",
    "- czułość na outlier'y\n",
    "- problem wielowymiarowych danych (klaster danych zbliżonych na jednym wymiarze nie będzie stworzony z racji na ogromne dystanse na innych wymiarach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. K-means with scikit-learn\n",
    "\n",
    "### 4.1. Download files mouse.csv and lines.csv. They have multiple examples described with 2 attributes.  You are given the functions to read files and plot the data. Use these functions to plot data from both files. Can you manually determine 3 clusters in each of the files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def read_file(path):\n",
    "    with open(path, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, quoting=csv.QUOTE_NONNUMERIC)\n",
    "        data = [row for row in reader]\n",
    "        data = StandardScaler().fit_transform(data)\n",
    "    return np.array(data)\n",
    "\n",
    "def plot_data(data):\n",
    "    plt.scatter(data[:,0], data[:, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO call functions above and try to find clusters in obtained datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Now, let's try to use k-means on the obtained dataset. Again, you are given a function to visualize the obtained plot. Your task is to use KMeans with propoer parameters on \"mouse\" and \"lines\" datasets and see if the clusters generated by k-means are the same that you suggested in the previous exercise.\n",
    "\n",
    "See documentation and examples: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters(clusters, centroids): \n",
    "    #clusters: list of numpy arrays (each array with examples in one cluster)\n",
    "    #centroids: numpy array\n",
    "    for c in clusters:\n",
    "        plt.scatter(c[:,0], c[:,1])\n",
    "    plt.scatter(centroids[:,0], centroids[:,1], marker='+', color='black', s=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# TODO use KMeans to cluster mouse and lines. Visualize and analyze the obtained clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Attribute selection in unsupervised problems\n",
    "\n",
    "As in unsupervised learning problems (such as clustering) we do not have decision classess, so attribute selection methods based on info gain, chi2 etc. are not applicable here.\n",
    "\n",
    "**Question** What can we do when we want to reduce the number of features in unsupervised problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to use two attribute selection methods, which can be used in clustering (unsupervised learning): removing low-variance attributes and removing correlated attributes.\n",
    "\n",
    "You are given code which reads real-world dataset containing data about customers and saves it in 'data' data frame. \n",
    "\n",
    "**Task** Perform attribute selection using the methods above. Firstly, you can use VarianceThreshold class (https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html). Try different values of threashold and compare how many attributes have been removed. Next, you can use corr() method of DataFrame to obtain the correlation matrix. What can we deduce from this matrix? How can we use it to feature selection? Try to implement proper attribute selection based on correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "data = pd.read_csv(\"customers.csv\")\n",
    "data = data.drop(columns=\"Address\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO play with attribute selection \n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "* Write your own implementation of k-means algorithm with random centroid initialization and 2 stopping conditions: max iterations and centroid convergence (if all attributes of all centroids changes not more than some epsilon the algorithm should stop). \n",
    "* Use your implementation to cluster data about cereal products with their dietary characteristics (cereals.csv, 16 attributes). \n",
    "* It contains some nominal attributes (name, mfr, type). You can omit the first two of them. Type attribute is binary, so you can replace it with values 0 and 1.\n",
    "* Perform the clustering of the cereals into 3 groups using k-means algorithm. \n",
    "* Remember to preprocess the data: normalization/standardization, attribute selection. \n",
    "* Try to describe the obtained groups based on the obtained centroids. What do all cereals within a certain group have in common?\n",
    "* Write a report containing information about preprocessing methods that you used, number of cereals within each cluster, and your conclusions about the clustering results.\n",
    "\n",
    "**Deadline +2 weeks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
